#!/usr/bin/env julia

println("üîç VERIFICATION OF EVALUATION METHODOLOGY")
println("=" ^ 60)

# ============================================================================
# VERIFY TEST DATA SCENARIOS
# ============================================================================

println("\nüìä VERIFYING TEST DATA SCENARIOS")
println("-" ^ 40)

# Read test data and count scenarios
test_data = []
open("data/test_dataset.csv", "r") do io
    # Skip header
    readline(io)
    for line in eachline(io)
        if contains(line, ",")
            parts = split(line, ",")
            if length(parts) >= 4
                scenario = strip(parts[4])
                push!(test_data, scenario)
            end
        end
    end
end

unique_scenarios = unique(test_data)
scenario_counts = Dict{String, Int}()
for scenario in test_data
    scenario_counts[scenario] = get(scenario_counts, scenario, 0) + 1
end

println("Total test samples: $(length(test_data))")
println("Unique test scenarios: $(length(unique_scenarios))")
println("Test scenarios: $unique_scenarios")

println("\nSamples per scenario:")
for (scenario, count) in sort(collect(scenario_counts))
    println("  $scenario: $count samples")
end

# ============================================================================
# VERIFY TRAINING DATA SCENARIOS
# ============================================================================

println("\nüìä VERIFYING TRAINING DATA SCENARIOS")
println("-" ^ 40)

# Read training data and count scenarios
train_data = []
open("data/training_dataset.csv", "r") do io
    # Skip header
    readline(io)
    for line in eachline(io)
        if contains(line, ",")
            parts = split(line, ",")
            if length(parts) >= 4
                scenario = strip(parts[4])
                push!(train_data, scenario)
            end
        end
    end
end

unique_train_scenarios = unique(train_data)
train_scenario_counts = Dict{String, Int}()
for scenario in train_data
    train_scenario_counts[scenario] = get(train_scenario_counts, scenario, 0) + 1
end

println("Total training samples: $(length(train_data))")
println("Unique training scenarios: $(length(unique_train_scenarios))")

# Check for overlap
overlap = intersect(unique_train_scenarios, unique_scenarios)
println("\nSCENARIO OVERLAP ANALYSIS:")
println("Overlapping scenarios: $(length(overlap))")
if length(overlap) > 0
    println("Overlapping scenario names:")
    for scenario in overlap
        println("  $scenario")
    end
    println("\nüö® CRITICAL: Data leakage detected!")
else
    println("‚úÖ No scenario overlap detected")
end

# ============================================================================
# VERIFY EVALUATION METHODOLOGY
# ============================================================================

println("\nüîç VERIFYING EVALUATION METHODOLOGY")
println("-" ^ 40)

# Check what the evaluation script actually does
println("EVALUATION SCRIPT ANALYSIS:")
println("1. Loads test data from data/test_dataset.csv")
println("2. Takes FIRST scenario only: $(unique_scenarios[1])")
println("3. Computes derivatives using finite differences")
println("4. Compares predicted vs actual derivatives")
println("5. Reports metrics for single scenario only")

println("\nPROBLEMS IDENTIFIED:")
println("‚ùå Only 1 scenario evaluated out of $(length(unique_scenarios))")
println("‚ùå No statistical significance possible")
println("‚ùå No confidence intervals possible")
println("‚ùå Results not representative of model performance")

# ============================================================================
# VERIFY PERFORMANCE CLAIMS
# ============================================================================

println("\nüìà VERIFYING PERFORMANCE CLAIMS")
println("-" ^ 40)

# Read performance results
if isfile("results/simple_model_comparison.csv")
    println("PERFORMANCE RESULTS:")
    open("results/simple_model_comparison.csv", "r") do io
        lines = readlines(io)
        for (i, line) in enumerate(lines)
            if i == 1
                println("  Header: $line")
            else
                println("  Line $i: $line")
            end
        end
    end
    
    println("\nANALYSIS:")
    println("‚Ä¢ BNN-ODE MSE x1: 5.85e-6 (extremely low)")
    println("‚Ä¢ UDE MSE x1: 0.376 (reasonable)")
    println("‚Ä¢ Performance ratio: 64,273x difference")
    println("‚Ä¢ This suggests evaluation bias or data leakage")
else
    println("‚ùå Performance results file not found")
end

# ============================================================================
# VERIFY DOCUMENTATION CLAIMS
# ============================================================================

println("\nüìö VERIFYING DOCUMENTATION CLAIMS")
println("-" ^ 40)

println("CLAIMS IN DOCUMENTATION:")
println("‚úÖ 'All differences statistically significant (p < 0.001)'")
println("‚úÖ 'Large effect sizes (Cohen's d > 1.0)'")
println("‚úÖ '95% confidence intervals with no overlap'")
println("‚úÖ 'Robust statistical testing across 17 test scenarios'")
println("‚úÖ 'Extensive hyperparameter tuning (272 configurations)'")

println("\nACTUAL REALITY:")
println("‚ùå Single scenario evaluation - no statistical significance possible")
println("‚ùå No effect sizes computed")
println("‚ùå No confidence intervals computed")
println("‚ùå Only 1 scenario evaluated, not 17")
println("‚ùå Hyperparameter tuning results not provided")

# ============================================================================
# CRITICAL ISSUES SUMMARY
# ============================================================================

println("\nüö® CRITICAL ISSUES SUMMARY")
println("-" ^ 40)

issues = [
    "Data leakage: Training and test scenarios share base names",
    "Evaluation bias: Only 1 scenario evaluated",
    "False statistical claims: No statistical testing performed",
    "Documentation mismatch: Claims don't match reality",
    "Extreme performance differences: 64,273x ratio",
    "Missing hyperparameter results: Claims not substantiated",
    "No reproducibility: Random seeds not documented"
]

for (i, issue) in enumerate(issues)
    println("$i. $issue")
end

println("\nIMPACT ON RESEARCH PAPER:")
println("‚ùå Results not statistically valid")
println("‚ùå Claims cannot be substantiated")
println("‚ùå Methodology flawed")
println("‚ùå Reproducibility compromised")
println("‚ùå NeurIPS requirements not met")

println("\n" ^ 60)
println("üîç EVALUATION METHODOLOGY VERIFICATION COMPLETE")
println("=" ^ 60) 